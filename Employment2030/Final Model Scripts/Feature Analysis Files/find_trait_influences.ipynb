{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tree_paths(tree):\n",
    "    \n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "    values = tree.value\n",
    "    \n",
    "    tree_paths = []\n",
    "    tree_probs = []\n",
    "    path = []\n",
    "    path_probs = []\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()    \n",
    "               \n",
    "        while len(path)>parent_depth+1:\n",
    "            path.pop()\n",
    "            path_probs.pop()\n",
    "        path.append(node_id)\n",
    "        true = values[node_id][0][0].copy()\n",
    "        false = values[node_id][0][1].copy()\n",
    "        path_probs.append(round(true/(true+false),3))\n",
    "        \n",
    "        # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            tree_paths.append(path.copy())\n",
    "            tree_probs.append(path_probs.copy())\n",
    "            \n",
    "    return tree_paths, tree_probs\n",
    "\n",
    "def forest_paths(model):    \n",
    "    forest_attributes = []\n",
    "    baseline_means = []\n",
    "    \n",
    "    for treeEst in model.estimators_:\n",
    "        path_thresholds = []\n",
    "        path_features = []\n",
    "        path_probs = []\n",
    "        \n",
    "        children_left = treeEst.tree_.children_left\n",
    "        children_right = treeEst.tree_.children_right\n",
    "        all_thresholds = treeEst.tree_.threshold\n",
    "        all_features = treeEst.tree_.feature\n",
    "\n",
    "        paths, path_probs = tree_paths(treeEst.tree_)\n",
    "        \n",
    "        leaf_probs = []\n",
    "        for prob_list in path_probs:\n",
    "                leaf_probs.append(prob_list[len(prob_list)-1])\n",
    "        baseline_means.append(np.mean(leaf_probs))\n",
    "\n",
    "        for i in range(len(paths)):\n",
    "            thresholds = []\n",
    "            features = []\n",
    "            for j in range(len(paths[i])-1):\n",
    "                if paths[i][j+1] == children_right[paths[i][j]]:\n",
    "                    thresholds.append(all_thresholds[paths[i][j]])\n",
    "                else:\n",
    "                    thresholds.append(-all_thresholds[paths[i][j]])\n",
    "                features.append(all_features[paths[i][j]])\n",
    "            path_thresholds.append(thresholds.copy())\n",
    "            path_features.append(features.copy())\n",
    "    \n",
    "        tree_atributes = pd.DataFrame([path_features, path_thresholds, path_probs]).T\n",
    "        tree_atributes.columns = ['features','thresholds','path_probs']\n",
    "        forest_attributes.append(tree_atributes.copy())\n",
    "    return forest_attributes, np.mean(baseline_means)\n",
    "\n",
    "def init_influence_list(features,conditionals,product):\n",
    "    if product:\n",
    "        combo_index = pd.MultiIndex.from_product([features,conditionals])\n",
    "    else:\n",
    "        combo_index = pd.MultiIndex.from_arrays([features,conditionals])\n",
    "    \n",
    "    infl_lists = []\n",
    "    for i in range(combo_index.to_series().shape[0]):\n",
    "        infl_lists.append([])\n",
    "    \n",
    "    influences = pd.Series(infl_lists,index=combo_index)\n",
    "    \n",
    "    return influences        \n",
    "\n",
    "def get_influences(feature_combos,model):\n",
    "    forest_attributes, baseline = forest_paths(model)\n",
    "    \n",
    "    for tree_frame in forest_attributes:\n",
    "        for index, path in tree_frame.iterrows():\n",
    "            previous = ['blank']\n",
    "            for step in range(len(path['features'])-1):\n",
    "                #what features we have seen and the feature we are on\n",
    "                current_feature = path['features'][step]\n",
    "                direction = np.sign(path['thresholds'][step])\n",
    "                if current_feature in feature_combos.index.get_level_values(0):\n",
    "                    #make relevant calculations\n",
    "                    current_prob = path['path_probs'][step]\n",
    "                    next_prob = path['path_probs'][step+1]\n",
    "                    pct_change = (next_prob-current_prob)/current_prob\n",
    "                    influence = direction*pct_change\n",
    "                    #add to all relevant combos\n",
    "                    for combo in feature_combos.loc[current_feature,previous]:\n",
    "                        combo.append(influence)\n",
    "                previous.append(current_feature*direction)\n",
    "                \n",
    "    influences_df = pd.DataFrame(index=feature_combos.index)\n",
    "    \n",
    "    for index, influences in feature_combos.iteritems():\n",
    "        if influences:\n",
    "            influences = np.asarray(influences)\n",
    "            influences_df.loc[index,'pos_influence'] = np.mean(influences[influences>0])\n",
    "            influences_df.loc[index,'neg_influence'] = np.mean(influences[influences<=0])\n",
    "            influences_df.loc[index,'pct_pos'] = np.sum(influences>0)/len(influences)\n",
    "            influences_df.loc[index,'occurance count'] = len(influences)\n",
    "    return influences_df\n",
    "\n",
    "def feature_name_index(table,features):\n",
    "    \n",
    "    conditional_labels = []#I know there is a better way to do this\n",
    "    for condition in table.index.get_level_values(1):\n",
    "        if condition == 'blank':\n",
    "            conditional_labels.append(condition)\n",
    "        elif condition >=0:\n",
    "            conditional_labels.append(\"high \"+features[condition])\n",
    "        else:\n",
    "            conditional_labels.append(\"low \"+features[-1*condition])\n",
    "    \n",
    "    table_index = pd.MultiIndex.from_arrays([\n",
    "        features[table.index.get_level_values(0)],conditional_labels])\n",
    "\n",
    "    table.index = table_index\n",
    "    \n",
    "    return table\n",
    "\n",
    "%run ../utils_rf\n",
    "\n",
    "#read training data\n",
    "file = \"../../tables/model_input/noc_answers.csv\"\n",
    "x, x_agg, y, y_agg, x_noclvl, y_noclvl = data_proccess(file,True)\n",
    "x.drop(['work_num_1','work_num_2','work_num_3','work_num_4','work_num_5','work_num_6'],axis=1,inplace=True)\n",
    "\n",
    "#grab just the noc codes to cut out test nocs from main noc table\n",
    "train_nocs = pd.read_csv(file,usecols=['noc_code']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#single time run\n",
    "rf = RandomForestClassifier(**init_params('cat'))\n",
    "rf.fit(x,y['increase'])\n",
    "\n",
    "#making combo sets to check\n",
    "conditionals = list(range(120))+list(range(-119,0))+['blank']\n",
    "all_combos = init_influence_list(range(120),conditionals,True)\n",
    "\n",
    "#running the analysis\n",
    "influences = feature_name_index(get_influences(all_combos,rf),x.columns)\n",
    "\n",
    "#pulling out the non conditional\n",
    "non_cond_inf = influences.xs('blank',level=1)\n",
    "non_cond_inf.to_csv(\"../../tables/feature_analysis_output/1run_non_conditional_influences.csv\")\n",
    "\n",
    "#getting portions instead of path counts\n",
    "influences = pd.merge(influences.reset_index(),\n",
    "      non_cond_inf['occurance count'].reset_index(),\n",
    "      left_on=['level_0'],\n",
    "      right_on=['index'],\n",
    "      how='inner').set_index(['level_0','level_1']).drop('index',axis=1)\n",
    "\n",
    "influences['occurance pct'] = influences['occurance count_x']/influences['occurance count_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#we are looking for consistency for our main outputs so we run the process 10 times\n",
    "#we for pairs we take a running average\n",
    "#for trying to find foundational skills we ask that they be above 0.95 positivie influence in all runs\n",
    "top10set = []\n",
    "for i in range(10):\n",
    "    rf = RandomForestClassifier(**init_params('cat')).fit(x,y['increase'])\n",
    "    conditionals = list(range(120))+list(range(-119,0))+['blank']\n",
    "    all_combos = init_influence_list(range(120),conditionals,True)\n",
    "    current_infl = feature_name_index(get_influences(all_combos,rf),x.columns).fillna(0)\n",
    "    \n",
    "    #tring to find our consistent foundational skills\n",
    "    non_cond_info = current_infl.xs('blank',level=1)\n",
    "    top10set.append(non_cond_info.loc[non_cond_info['pct_pos']>0.95].copy())\n",
    "    \n",
    "    pairs = current_infl[\n",
    "    np.logical_not(\n",
    "        np.in1d(current_infl.index.get_level_values(1), 'blank')\n",
    "    )].copy()\n",
    "    \n",
    "    current_sig_pairs = pairs.loc[np.logical_or(pairs['pct_pos']>0.95,pairs['pct_pos']<0.05)].copy()\n",
    "    current_sig_pairs['count']=1\n",
    "    if i==0:\n",
    "        sig_pairs = current_sig_pairs\n",
    "    else:\n",
    "        common_idx = sig_pairs.index.intersection(current_sig_pairs.index) #grab the pair we have seen\n",
    "        for_update = sig_pairs.loc[common_idx,sig_pairs.columns != 'count'].copy()#make lists of old and new info\n",
    "        update_with = current_sig_pairs.loc[common_idx,sig_pairs.columns != 'count'].copy()\n",
    "        new_pairs = current_sig_pairs.loc[~current_sig_pairs.index.isin(common_idx)].copy()#new rows\n",
    "        counts = sig_pairs.loc[common_idx,'count'].copy()\n",
    "        updated  = (for_update.mul(counts,axis=0).add(update_with,axis=0)).div(counts+1,axis=0)\n",
    "        \n",
    "        sig_pairs.loc[common_idx,sig_pairs.columns != 'count'] = updated.copy()\n",
    "        sig_pairs.loc[common_idx,'count'] = sig_pairs.loc[common_idx,'count']+1\n",
    "        sig_pairs = pd.concat([sig_pairs,new_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset = []\n",
    "for bestset in top10set:\n",
    "    fullset.append(bestset.index.get_values())\n",
    "    \n",
    "cons_set = np.unique(np.concatenate(fullset),return_counts =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-b245c73c4f81>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-b245c73c4f81>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    cons_set[]\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cons_set[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define signficance markers based on how many times we ran everything\n",
    "sig_pairs['significance'] = 'not sig'\n",
    "sig_pairs.loc[sig_pairs['count']>i/4,'significance'] = '*'\n",
    "sig_pairs.loc[sig_pairs['count']>2*i/4,'significance'] = '**'\n",
    "sig_pairs.loc[sig_pairs['count']>3*i/4,'significance'] = '***'\n",
    "sig_pairs = sig_pairs.loc[sig_pairs['significance']!='not sig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name index levels\n",
    "sig_pairs.index.set_names(['main','conditional'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_pairs.to_csv('../../tables/feature_analysis_output/sig_pairs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
