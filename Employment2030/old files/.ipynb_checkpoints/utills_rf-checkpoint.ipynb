{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, f_regression, mutual_info_regression, f_classif, SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, log_loss\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Processing Data*\n",
    "\n",
    "the below function reads the data file and creats 3 tables:\n",
    "    1. the x table - workshop number and rounded SAk scores\n",
    "    2. aggregated x table - noc and sak level\n",
    "    3. y with individual expert answers, a vector for binning on increase, decrease and no bin\n",
    "    4. aggregated y - vector for binned on increase, decrease and  non-binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_proccess(file):\n",
    "    data = pd.read_csv(file,index_col=['noc','workshop.number'])\n",
    "    data.sort_index(inplace=True)\n",
    "    data.loc[data.share == 'remain constant','share'] = 'constant'\n",
    "        \n",
    "    x = data.drop(['absolute','share','Unnamed: 0','noc_code'],axis=1) #making x data frame\n",
    "    x['work_num'] = x.index.get_level_values(1) #making workshop number a variable as well as an index\n",
    "    x = np.round(x).astype(int)#round x to make discrete\n",
    "    \n",
    "    x_agg = x.drop_duplicates()\n",
    "    \n",
    "    x_noclvl = x_agg.drop('work_num',axis=1).droplevel(1).drop_duplicates()\n",
    "    \n",
    "    y = pd.DataFrame({'non_binned': data['share'],\n",
    "              'increase': data['share'].str.replace('constant','decrease'),\n",
    "              'decrease': data['share'].str.replace('constant','increase')})\n",
    "\n",
    "    y_agg = pd.DataFrame(data['share']).pivot_table(index = ['noc','workshop.number'], columns = 'share', aggfunc = len).fillna(0)\n",
    "    y_agg['sum'] = y_agg.sum(axis = 1)\n",
    "    y_noclvl = y_agg.groupby(level=0).sum()\n",
    "    y_agg.loc[:,y_agg.columns!='sum'] = y_agg.loc[:,y_agg.columns!='sum'].divide(y_agg['sum'],axis=0)\n",
    "    y_noclvl.loc[:,y_noclvl.columns!='sum'] = y_noclvl.loc[:,y_noclvl.columns!='sum'].divide(y_noclvl['sum'],axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return x, x_agg, y, y_agg, x_noclvl, y_noclvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(model_type):\n",
    "    if model_type == 'cat':\n",
    "        params = {\n",
    "         'criterion': 'gini',\n",
    "         'max_features': 'auto',\n",
    "         'min_samples_leaf': 8,\n",
    "         'min_samples_split': 5,\n",
    "         'n_estimators': 1000,\n",
    "         'n_jobs':-1\n",
    "        }\n",
    "    if model_type == 'reg':\n",
    "        params = {\n",
    "         'criterion': 'mse',\n",
    "         'max_features': None,\n",
    "         'min_samples_leaf': 1,\n",
    "         'min_samples_split': 15,\n",
    "         'n_estimators': 250,\n",
    "         'n_jobs':-1\n",
    "        }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold (x,y,params,index,binned,model_type):\n",
    "    \n",
    "    x = pd.DataFrame(x)\n",
    "    \n",
    "    rf = RandomForestClassifier(**params)\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    n_trees = params['n_estimators']\n",
    "    \n",
    "    if model_type == 'reg':\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        kf = KFold(n_splits=10,shuffle=True)\n",
    "    \n",
    "    if binned:\n",
    "        pred = np.zeros(x.shape[0])\n",
    "    else:\n",
    "        pred = np.zeros((x.shape[0],3))\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        \n",
    "        if model_type == 'reg':\n",
    "            pred[test_index] = rf.predict(x_test)\n",
    "        if model_type == 'pred_probs':\n",
    "            if binned:\n",
    "                pred[test_index] = rf.predict_proba(x_test)[:,1]\n",
    "            else: \n",
    "                pred[test_index] = rf.predict_proba(x_test)\n",
    "        if model_type == 'tree_port':\n",
    "            tree_pred = np.zeros((n_trees,len(test_index))) \n",
    "            for tree in range(n_trees):\n",
    "                tree_pred[tree] = rf.estimators_[tree].predict(x_test)\n",
    "            pred[test_index] = tree_pred.mean(axis=0)\n",
    "    \n",
    "    pred = pd.DataFrame(pred,index=index).groupby(index).first()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_feature_importance(x,y,model_type):\n",
    "    rf = RandomForestClassifier(**init_params(model_type))\n",
    "    kf = KFold(n_splits=10,shuffle=False)\n",
    "    \n",
    "    if model_type == 'reg':\n",
    "        rf = RandomForestRegressor(**init_params(model_type))\n",
    "        kf = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    feature_imp = np.zeros((x.shape[1],5))\n",
    "    i=0\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        rf.fit(x_train,y_train)\n",
    "        \n",
    "        feature_imp[:,i] = rf.feature_importances_\n",
    "    \n",
    "    return feature_imp.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_search(x,y,model_type):\n",
    "    \n",
    "    param_grid= {'n_estimators':[100,150,250,275,300,600,1000],#number of trees \n",
    "             'min_samples_leaf': [1,2,4,8],#minimum number of data points can be used to make a leaf at the end of a tree \n",
    "             'min_samples_split': [5,10,15]#min number of data points to split a branch \n",
    "             }\n",
    "    \n",
    "    \n",
    "    if model_type == 'reg':\n",
    "        rf = RandomForestRegressor(**init_params(model_type))\n",
    "        search = GridSearchCV(rf,param_grid,scoring='neg_mean_squared_error',cv=5,n_jobs=-1,iid=False)\n",
    "        \n",
    "    if model_type == 'cat':\n",
    "        rf = RandomForestClassifier(**init_params(model_type))\n",
    "        param_grid['criterion'] = ['gini','entropy']\n",
    "        search = GridSearchCV(rf,param_grid,scoring='neg_log_loss',cv=5,n_jobs=-1,iid=False)\n",
    "    \n",
    "    search.fit(x,y)\n",
    "    \n",
    "    return search.best_params_, search.cv_results_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_feature_selection(x,y,model_type,k):\n",
    "    if model_type == 'class':\n",
    "        return SelectKBest(mutual_info_classif,k).fit_transform(x,y)\n",
    "\n",
    "    if model_type == 'reg':\n",
    "        return SelectKBest(mutual_info_regression,k).fit_transform(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scores for a number of differnt measures\n",
    "def different_feature_rankings(x, x_agg, y, y_agg):\n",
    "    mi_class, chi2_class, f_class = basic_feature_selection(x,y['increase'],'class')\n",
    "    mi_reg, f_reg = basic_feature_selection(x_agg,y_agg['increase'],'reg')\n",
    "    \n",
    "    feature_scores = pd.DataFrame({'mi_class': mi_class.scores_,\n",
    "                 'chi2': chi2_class.scores_,\n",
    "                 'f_class': f_class.scores_,\n",
    "                 'mi_reg': mi_reg.scores_,\n",
    "                 'f_reg': f_reg.scores_},index=x.columns)\n",
    "    \n",
    "    feature_scores.sort_values('mi_reg',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_by_k(x, x_agg, y, y_agg):\n",
    "    reg_scores = np.zeros((120,120))\n",
    "    class_scores = np.zeros((120,120))\n",
    "\n",
    "    \n",
    "    \n",
    "    for k in range(1,121):\n",
    "        reg_scores[k-1] = run_k_fold(basic_feature_selection(x_agg,y_agg['increase'],'reg',k),\n",
    "                            y_agg['increase'],\n",
    "                            init_params('reg'),\n",
    "                            x_agg.index,\n",
    "                            True,'reg').iloc[:,0].values\n",
    "        class_scores[k-1] = run_k_fold(basic_feature_selection(x,y['increase'],'class',k),\n",
    "                            y['increase'],\n",
    "                            init_params('cat'),\n",
    "                            x.index,\n",
    "                            True,'pred_probs').iloc[:,0].values\n",
    "        \n",
    "    r_scores = abs(pd.DataFrame(reg_scores,index=range(1,121),columns=x_agg.index).T.subtract(y_agg['increase'],axis=0)).mean(axis=0)\n",
    "    c_scores = abs(pd.DataFrame(class_scores,index=range(1,121),columns=x_agg.index).T.subtract(y_agg['increase'],axis=0)).mean()\n",
    "        \n",
    "    return r_scores, c_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_models(x, x_agg, y, y_agg,binned,increase,k_reg,k_class):    \n",
    "    \n",
    "    if binned:\n",
    "        if increase:\n",
    "            #set number of features to run with\n",
    "            x_agg_cut = basic_feature_selection(x_agg,y_agg['increase'],'reg',k_reg)\n",
    "            x_cut = basic_feature_selection(x,y['increase'],'class',k_class)\n",
    "\n",
    "            #run regression in a k-fold framework and place results into dataframes\n",
    "            pred = pd.concat([\n",
    "                run_k_fold(x_agg_cut,y_agg['increase'],init_params('reg'),x_agg.index,binned,'reg'),\n",
    "                run_k_fold(x_cut,y['increase'],init_params('cat'),x.index,binned,'pred_probs'),\n",
    "                run_k_fold(x_cut,y['increase'],init_params('cat'),x.index,binned,'tree_port')\n",
    "            ],axis=1)\n",
    "\n",
    "        else:\n",
    "            x_agg_cut = basic_feature_selection(x_agg,y_agg['decrease'],'reg',k_reg)\n",
    "            x_cut = basic_feature_selection(x,y['decrease'],'class',k_class)\n",
    "\n",
    "            #run regression in a k-fold framework and place results into dataframes\n",
    "            pred = pd.concat([\n",
    "                run_k_fold(x_agg_cut,y_agg['decrease'],init_params('reg'),x_agg.index,binned,'reg'),\n",
    "                1 - run_k_fold(x_cut,y['decrease'],init_params('cat'),x.index,binned,'pred_probs'),\n",
    "                1 - run_k_fold(x_cut,y['decrease'],init_params('cat'),x.index,binned,'tree_port')\n",
    "            ],axis=1)\n",
    "        \n",
    "    else:\n",
    "        y_agg = y_agg[['constant','decrease','increase']]\n",
    "        y = y['non_binned']\n",
    "        pred = pd.concat([\n",
    "            run_k_fold(x_agg,y_agg,init_params('reg'),x_agg.index,binned,'reg'),\n",
    "            run_k_fold(x,y,init_params('cat'),x.index,binned,'pred_probs')#,\n",
    "            #run_k_fold(x,y,init_params('cat'),x.index,binned,'tree_port')\n",
    "        ],axis=1)\n",
    "        \n",
    "    \n",
    "    pred.set_index(x_agg.index,inplace = True)\n",
    "    if binned:\n",
    "        pred.columns = ['regression','pred_prob','tree_portions']\n",
    "    else:\n",
    "        pred.columns = ['regression_con','regression_dec','regression_inc',\n",
    "                                 'prob_con','prob_dec','prob_inc']\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(pred,truth,binned):\n",
    "    if binned:\n",
    "        matrix = pd.DataFrame(\n",
    "         [[sum(np.logical_and(truth>=0.5,pred>=0.5)),\n",
    "           sum(np.logical_and(truth>=0.5,pred<0.5))],\n",
    "          [sum(np.logical_and(truth<0.5,pred>=0.5)),\n",
    "           sum(np.logical_and(truth<0.5,pred<0.5))]]\n",
    "        ,columns=['pred_increase','pred_decrease'],index=['true_increase','true_decrease'])\n",
    "    else:\n",
    "        pred.columns = ['constant','decrease','increase']\n",
    "        matrix = pd.DataFrame(\n",
    "        [[sum(np.logical_and(truth.idxmax(axis=1)=='increase',pred.idxmax(axis=1)=='increase')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='increase',pred.idxmax(axis=1)=='constant')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='increase',pred.idxmax(axis=1)=='decrease'))],\n",
    "         [sum(np.logical_and(truth.idxmax(axis=1)=='constant',pred.idxmax(axis=1)=='increase')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='constant',pred.idxmax(axis=1)=='constant')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='constant',pred.idxmax(axis=1)=='decrease'))],\n",
    "         [sum(np.logical_and(truth.idxmax(axis=1)=='decrease',pred.idxmax(axis=1)=='increase')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='decrease',pred.idxmax(axis=1)=='constant')),\n",
    "          sum(np.logical_and(truth.idxmax(axis=1)=='decrease',pred.idxmax(axis=1)=='decrease'))]],\n",
    "        columns=['pred_increase','pred_constant','pred_decrease'],index=['true_increase','true_constant','true_decrease'])\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sfs(x,y,model_type):\n",
    "    if model_type == 'reg':\n",
    "        rf = RandomForestRegressor(**init_params(model_type))\n",
    "        sfs = SFS(rf, \n",
    "           k_features=(5,110), \n",
    "           forward=True, \n",
    "           floating=True, \n",
    "           verbose=2,\n",
    "           scoring='neg_mean_squared_error',\n",
    "           cv=5,\n",
    "           n_jobs=-1)\n",
    "    else:\n",
    "        rf = RandomForestClassifier(**init_params(model_type))\n",
    "        sfs = SFS(rf, \n",
    "           k_features=(1,20), \n",
    "           forward=True, \n",
    "           floating=True, \n",
    "           verbose=2,\n",
    "           scoring=make_scorer(custom_MSE,greater_is_better=False,needs_proba=True),\n",
    "           cv=5,\n",
    "           n_jobs=-1)\n",
    "    \n",
    "    sfs.fit(x,y)\n",
    "    \n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_MAE(y_true, y_pred):\n",
    "    totals = y_true.groupby(level=[0,1]).count()\n",
    "    increase_count = y_true[y_true=='increase'].groupby(level=[0,1]).count()\n",
    "    y_true_agg = increase_count.divide(totals).fillna(0)\n",
    "    \n",
    "    y_true_agg_rep = np.repeat(y_true_agg[0],totals[0])\n",
    "    for i in range(1,len(y_true_agg)):\n",
    "        y_true_agg_rep = np.concatenate((y_true_agg_rep,np.repeat(y_true_agg[i],totals[i])),axis=None)\n",
    "    \n",
    "    return mean_absolute_error(y_true_agg_rep,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
