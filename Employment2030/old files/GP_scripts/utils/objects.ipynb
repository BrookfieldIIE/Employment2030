{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 14:26:11.805272 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/session_manager.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0908 14:26:11.809288 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/misc.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0908 14:26:12.030620 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "W0908 14:26:12.031362 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/training/tensorflow_optimizer.py:156: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0908 14:26:12.032185 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\n",
      "\n",
      "W0908 14:26:12.034456 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "W0908 14:26:12.037482 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0908 14:26:12.069056 4489426368 deprecation_wrapper.py:119] From /Users/rob/.pyenv/versions/brookfield/lib/python3.5/site-packages/gpflow/saver/coders.py:80: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import gpflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, confusion_matrix\n",
    "\n",
    "from ordinal_likelihood import Ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles each dataset, both the x and the y. This is done mainly for grid search \n",
    "# reasons as it allows the creation of lists of datasets and the automatic transformation\n",
    "# of them into the right form for GPFlow\n",
    "class GaussianDataset():\n",
    "    # x: DataFrame or Numpy array, the data features\n",
    "    # y_labels: DataFrame or Numpy array, the data labels (2 columns, 1st: distribution, 2nd: class)\n",
    "    # group: Array, the noc label for each skill vector\n",
    "    # agg_level: String, 'agg' or 'ind' for whether the data is aggregated\n",
    "    # value_type: String, 'disc' or 'cont' (discrete or continuous data)\n",
    "    # binned: String, 'binned' or 'not_binned' whether the data labels are binned\n",
    "    def __init__(self, x, y_labels, y_true_dist, group, agg_level, value_type, binned):\n",
    "        # Tranformation of string data into integer data\n",
    "        self.data_transforms = change = {'decrease':     2,\n",
    "                                         'constant':     1, \n",
    "                                         'increase':     0,\n",
    "                                         'fewer':        2,\n",
    "                                         'same':         1,\n",
    "                                         'more':         0,\n",
    "                                         'not_increase': 1}\n",
    "        # Transformations necessary to get in proper TensorFlow format\n",
    "        self.x = np.array(x)\n",
    "        # This converts the target data into a pandas Series, tarnsforms the type to int \n",
    "        # and then casts it as 'int64' (because evidently that is different than pythons\n",
    "        # built in int). That series is then transformed into a numpy array before being\n",
    "        # reshaped. I'm not exactly sure why all of these steps were necessary but \n",
    "        # TensorFlow was very finicky. \n",
    "        self.y = np.array(pd.Series(y_labels)\n",
    "                          .replace(self.data_transforms)\n",
    "                          .values.astype('int64')\n",
    "                         ).reshape(y_labels.shape[0], 1)\n",
    "        # Characteristics of the data\n",
    "        self.y_true_dist = y_true_dist\n",
    "        self.group = group\n",
    "        self.agg_level = agg_level\n",
    "        self.value_type = value_type\n",
    "        self.binned = binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles one set of results from a testing cycle of the model\n",
    "class Result():\n",
    "    # y_true_dist: Array of Tuples, true confidence in each label\n",
    "    # y_pred_dist: Array of Tuples, predicted confidence in each label\n",
    "    def __init__(self, y_true_dist, y_pred_dist):\n",
    "        self.y_true_dist = y_true_dist\n",
    "        self.y_pred_dist = y_pred_dist\n",
    "        self.y_true_class = [np.argmax(dist) for dist in y_true_dist]\n",
    "        self.y_pred_class = [np.argmax(dist) for dist in y_pred_dist]\n",
    "        self.mse = mean_squared_error(y_true_dist, y_pred_dist)\n",
    "        self.mae = mean_absolute_error(y_true_dist, y_pred_dist)\n",
    "        self.roc = self.multiclass_roc_auc_score()\n",
    "        self.confusion = confusion_matrix(self.y_true_class, self.y_pred_class)\n",
    "    \n",
    "    # From stackoverflow: \n",
    "    # https://stackoverflow.com/questions/39685740/calculate-sklearn-roc-auc-score-for-multi-class\n",
    "    # Baseline assumption is that the average of one-vs-all for each of the classes will be\n",
    "    # similar to the ROC-AUC score for a binary label.\n",
    "    def multiclass_roc_auc_score(self, average=\"macro\"):\n",
    "        # Creating a set of all the unique classes using the actual class list\n",
    "        unique_class = set(self.y_true_class)\n",
    "        class_roc_auc = []\n",
    "        for per_class in unique_class:\n",
    "            # Creating a list of all the classes except the current class \n",
    "            other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "            # Marking the current class as 1 and all other classes as 0\n",
    "            new_actual_class = [0 if x in other_class else 1 for x in self.y_true_class]\n",
    "            new_pred_class = [0 if x in other_class else 1 for x in self.y_pred_class]\n",
    "\n",
    "            # Using the sklearn metrics method to calculate the roc_auc_score\n",
    "            roc_auc = roc_auc_score(new_actual_class, new_pred_class, average=average)\n",
    "            class_roc_auc.append(roc_auc)\n",
    "\n",
    "        return sum(class_roc_auc)/len(class_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles all of the aspects of the model from building to testing\n",
    "class GaussianModel():\n",
    "    # x_train: x vector containing the training data\n",
    "    # y_train: y vector containing the training labels\n",
    "    # kernel: String, representing the kernel type\n",
    "    def __init__(self, x_train, y_train, kernel, n_classes):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.kernel_name = kernel\n",
    "        self.n_classes = n_classes\n",
    "        self.model = None\n",
    "        self.results = None\n",
    "    \n",
    "    # Builds, trains and then tests the model. Expects y_test to be a vector of the \n",
    "    # predicted distributions over the potential labels of y\n",
    "    def build_train_test(self, x_test, y_test):\n",
    "        self.build_train()\n",
    "        return self.test_model(x_test, y_test)\n",
    "    \n",
    "    ### Functions for building and training the model ###\n",
    "    \n",
    "    # This builds and trains the model, could potentially be done asynchronously \n",
    "    def build_train(self):\n",
    "        # Building the components of the model\n",
    "        kernel = self.get_kern(self.x_train.shape[1])\n",
    "        likelihood = self.create_likelihood()\n",
    "        # Build the model itself from GPFlow. Again, TensorFlow is finicky and require the\n",
    "        # x matrix to be cast to tf.float64 at this stage rather than any stage beforehand.\n",
    "        gaussian_model = gpflow.models.VGP(tf.cast(self.x_train, tf.float64),\n",
    "                                           self.y_train, \n",
    "                                           kern=kernel,\n",
    "                                           likelihood=likelihood)\n",
    "        # Train the model using Scipy (recommended from GPFlow documentation)\n",
    "        gpflow.train.ScipyOptimizer().minimize(gaussian_model)\n",
    "        self.model = gaussian_model\n",
    "    \n",
    "    # Creates the kernel for the model, this is based off a string passed in as a param \n",
    "    # during object creation. Can be expanded with more kernels offered by GPFlow.\n",
    "    def get_kern(self, dims):\n",
    "        # Assumption: kernel will be 'xxx_yyy' where yyy is Linear or does not exist\n",
    "        kern = self.kernel_name.split('_')\n",
    "        with gpflow.defer_build():\n",
    "            # Dims here defines the columns that the kernel looks at\n",
    "            if kern[0] == 'Matern12': kernel = gpflow.kernels.Matern12(input_dim=dims)\n",
    "            if kern[0] == 'Matern32': kernel = gpflow.kernels.Matern32(input_dim=dims) \n",
    "            if kern[0] == 'Matern52': kernel = gpflow.kernels.Matern52(input_dim=dims) \n",
    "            if kern[0] == 'RBF':      kernel = gpflow.kernels.RBF(input_dim=dims) \n",
    "            # Prior decided because it performed the best and was used by Nesta\n",
    "            kernel.variance.prior = gpflow.priors.Gamma(scale=1,shape=self.n_classes)\n",
    "            # This handles the case where the kernels are summed together\n",
    "            if len(kern) == 2 and kern[1] == 'Linear':\n",
    "                linear = gpflow.kernels.Linear(input_dim=dims)\n",
    "                linear.variance.prior = gpflow.priors.Gamma(scale=1,shape=self.n_classes)\n",
    "                kernel += linear\n",
    "        return kernel\n",
    "    \n",
    "    # Creation of the likelihood can be done with either GPFlow or the code from Johnathon.\n",
    "    # Bin edges are important because they define the categories for the predictions since\n",
    "    # a GP simply returns a predicted value on a continuous scale.\n",
    "    def create_likelihood(self):\n",
    "        bin_edges = np.array(np.arange(self.n_classes + 1), dtype=float)\n",
    "        bin_edges = bin_edges - .5\n",
    "        # This is the new ordinal likelihood from Johnathon\n",
    "        return Ordinal(bin_edges)\n",
    "    \n",
    "    ### Functions for testing the model ###\n",
    "    \n",
    "    # This creates predictions for each of the rows in x_test and then creates a results \n",
    "    # object that is saved in state.\n",
    "    def test_model(self, x_test, y_test):\n",
    "        densities = []\n",
    "        # Predictive density (i.e. the confidence in each category) for a single input x\n",
    "        for x in x_test:\n",
    "            # Predictions need to be scaled to 100% because there are theoretically infinite\n",
    "            # classes that it could predict.\n",
    "            densities.append(self.scale_pred(self.predictive_density(x)))\n",
    "        self.results = Result(y_test, densities)\n",
    "        return self.results\n",
    "    \n",
    "    # Create the prediction for each category, from:\n",
    "    # https://gpflow.readthedocs.io/en/latest/notebooks/ordinal.html\n",
    "    def predictive_density(self, x):\n",
    "        ys = np.arange(np.max(self.model.Y.value+1)).reshape([-1, 1])\n",
    "        x_new_vec = x*np.ones_like(ys)\n",
    "        # For predict_density x and y need to have the same number of rows\n",
    "        densities = np.exp(self.model.predict_density(x_new_vec, ys))\n",
    "        # Need to unpack the densities from [[i], [j], [k]]\n",
    "        densities = [d for [d] in densities]\n",
    "        return densities\n",
    "    \n",
    "    ### Functions for predicting on new data ### \n",
    "    \n",
    "    def predict_new_nocs(self, nocs):\n",
    "        mappings = {0: 'increase',\n",
    "                    1: 'constant/not increase',\n",
    "                    2: 'decrease'}\n",
    "        predictions = {}\n",
    "        classes = defaultdict(int)\n",
    "        for index, row in nocs.iterrows():\n",
    "            prediction = self.scale_pred(self.predictive_density(np.asarray(row)))\n",
    "            category = mappings[np.argmax(prediction)]\n",
    "            predictions[index] = {'Category':     category,\n",
    "                                  'Distribution': prediction}\n",
    "            classes[category] += 1\n",
    "        return predictions, classes\n",
    "    \n",
    "    ### Utility functions ###\n",
    "    \n",
    "    # Scales the predictions to each 100%\n",
    "    def scale_pred(self, pred):\n",
    "        factor = 1/sum(pred)\n",
    "        scaled = []\n",
    "        for p in pred:\n",
    "            scaled.append(p * factor)\n",
    "        return tuple(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
