{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ../../data_processing.ipynb\n",
    "%run ../utils/objects.ipynb\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive default dictionary, used for nested defaultdicts\n",
    "def rec_dd():\n",
    "    return defaultdict(rec_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cross validation process a default of 5 times. Results are a list of result objects,\n",
    "# one for each of the cv runs\n",
    "def cv(data, kernel, splits=5):\n",
    "    kf = GroupKFold(n_splits=splits)\n",
    "    results = []\n",
    "    if data.binned == 'binned': n_classes = 2\n",
    "    else:                       n_classes = 3\n",
    "\n",
    "    for train_index, test_index in kf.split(data.x, groups=data.group):\n",
    "        start = time.time()\n",
    "        x_train, x_test = data.x[train_index], data.x[test_index]\n",
    "        y_train, y_test = data.y[train_index], data.y_true_dist[test_index]\n",
    "        model = GaussianModel(x_train, y_train, kernel, n_classes)\n",
    "        results.append(model.build_train_test(x_test, y_test))\n",
    "        end = time.time()\n",
    "        print('Fold tested in (sec):', end - start)\n",
    "        # This is needed because (I think) GPFlow isn't handling garbage collection \n",
    "        # properly as the time it takes to train a brand new model, even after deleting\n",
    "        # all previous models, increases linearly based on the number of models trained\n",
    "        # since the kernel was last restarted. However, this doesn't completely fix the \n",
    "        # problem, just makes it a bit better\n",
    "        del model\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumps the results in a pickle file in the results folder\n",
    "def pickle_results(results, params):\n",
    "    with open('../gp_results/{}.pkl'.format(params), 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True prediction distribution for aggregated, binned share data\n",
    "y_share_agg_pred_dist = y_share_agg[['increase', 'not_increase']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True prediction distribution for individual, binned share data\n",
    "y_share_ind_pred_dist = []\n",
    "for noc in ind_nocs:\n",
    "    index = agg_nocs.index(noc)\n",
    "    y_share_ind_pred_dist.append(y_share_agg_pred_dist[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not including absolute or non_binned datasets here even though they are created\n",
    "agg_cont_binned = GaussianDataset(x_cont_agg, \n",
    "                    y_share_agg['binned_y'],\n",
    "                    y_share_agg_pred_dist,\n",
    "                    agg_nocs,\n",
    "                    'agg',\n",
    "                    'cont',\n",
    "                    'binned')\n",
    "agg_disc_binned = GaussianDataset(x_disc_agg, \n",
    "                    y_share_agg['binned_y'],\n",
    "                    y_share_agg_pred_dist,\n",
    "                    agg_nocs,\n",
    "                    'agg',\n",
    "                    'disc',\n",
    "                    'binned')\n",
    "ind_cont_binned = GaussianDataset(x_cont_ind, \n",
    "                    y_share_bin_ind,\n",
    "                    y_share_ind_pred_dist,\n",
    "                    ind_nocs,\n",
    "                    'ind',\n",
    "                    'cont',\n",
    "                    'binned')\n",
    "ind_disc_binned = GaussianDataset(x_disc_ind, \n",
    "                    y_share_bin_ind,\n",
    "                    y_share_ind_pred_dist,\n",
    "                    ind_nocs,\n",
    "                    'ind',\n",
    "                    'disc',\n",
    "                    'binned')\n",
    "\n",
    "datasets = [agg_cont_binned, agg_disc_binned, ind_cont_binned, ind_disc_binned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
